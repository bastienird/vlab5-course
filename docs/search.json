[
  {
    "objectID": "cwp-standards-first-draft.html#introduction-to-the-cwp",
    "href": "cwp-standards-first-draft.html#introduction-to-the-cwp",
    "title": "Role of CWP Standards in GTA Draft",
    "section": "1. Introduction to the CWP",
    "text": "1. Introduction to the CWP\nThe Coordinating Working Party on Fishery Statistics (CWP), under FAO’s leadership, is the main international forum for agreeing on:\n\nCommon definitions, classifications, and standards for capturing fishery and aquaculture statistics.\nIt develops methods, reduces workload for national statistical offices, and promotes harmonization across regions. https://openknowledge.fao.org/server/api/core/bitstreams/a7f3993f-1af1-4e65-8b91-036d12a34458/content"
  },
  {
    "objectID": "cwp-standards-first-draft.html#what-cwp-offers---handbook-best-practices",
    "href": "cwp-standards-first-draft.html#what-cwp-offers---handbook-best-practices",
    "title": "Role of CWP Standards in GTA Draft",
    "section": "2. What CWP offers - Handbook & best practices",
    "text": "2. What CWP offers - Handbook & best practices\nThe CWP Handbook of Fishery Statistics provides:\n\nStandard terminology for catch, fishing effort, and aquaculture data.\nClassifications like the ASFIS list (species codes), and structured data exchange protocols.\n\nIt also shares best practices in:\n\nHarmonization, geospatial standards, managing sensitive/confidential data, and workflows."
  },
  {
    "objectID": "cwp-standards-first-draft.html#alignment-with-gta-workflows",
    "href": "cwp-standards-first-draft.html#alignment-with-gta-workflows",
    "title": "Role of CWP Standards in GTA Draft",
    "section": "3. Alignment with GTA workflows",
    "text": "3. Alignment with GTA workflows\nThe GTA builds on CWP foundations by:\n\nApplying harmonized vocabularies and species coding systems in Level 0 datasets.\nStructuring processed outputs using standardized classifications for interoperability.\n\nThis ensures that GTA outputs are FAIR-aligned and can integrate seamlessly with global fisheries systems."
  },
  {
    "objectID": "datasetworkflowarchitecture-all.html",
    "href": "datasetworkflowarchitecture-all.html",
    "title": "Global Tuna Atlas datasets and workflow architecture",
    "section": "",
    "text": "The Global Tuna Atlas (GTA) integrates catch and effort data from all five tuna Regional Fisheries Management Organizations (t-RFMOs). GTA workflows aim to make tuna fisheries data FAIR (Findable, Accessible, Interoperable, Reusable) and reproducible.\nThe dataset families include:\n\nNominal catches (annual, aggregated by fleet, gear, species, large area).\nGeo-referenced catches (monthly, 1°/5° grids).\nEffort datasets (multiple measurement units, kept unaggregated to preserve semantics).\nDerived CPUE datasets (catch-per-unit-effort on matched strata).\n\nEach dataset is produced using harmonized formats based on CWP standards and reproducible scripts available in:\n\nLevel 2 & Effort (IRD): https://github.com/firms-gta/geoflow-tunaatlas\n\nLevel 0 (FIRMS): https://github.com/firms-gta/geoflow-gta"
  },
  {
    "objectID": "datasetworkflowarchitecture-all.html#nominal-catch-dataset-firms-level-0",
    "href": "datasetworkflowarchitecture-all.html#nominal-catch-dataset-firms-level-0",
    "title": "Global Tuna Atlas datasets and workflow architecture",
    "section": "Nominal Catch Dataset (FIRMS Level 0)",
    "text": "Nominal Catch Dataset (FIRMS Level 0)\n\nTime span: 1918–2023\n\nContent: Live-weight equivalent (metric tonnes), mainly retained catches\n\nStratification: Year, fleet, gear, large area, species\n\nUse case: Benchmark of global tuna catch volumes\n\nDOI: https://doi.org/10.5281/zenodo.5745958"
  },
  {
    "objectID": "datasetworkflowarchitecture-all.html#geo-referenced-catch-datasets",
    "href": "datasetworkflowarchitecture-all.html#geo-referenced-catch-datasets",
    "title": "Global Tuna Atlas datasets and workflow architecture",
    "section": "Geo-referenced Catch Datasets",
    "text": "Geo-referenced Catch Datasets\n\nFIRMS Level 0\n\nTime span: 1950–2023\n\nResolution: 1°/5° grid, monthly\n\nContent: Catches from all t-RFMOs, harmonized\n\nDOI: https://doi.org/10.5281/zenodo.5747174\n\n\n\nIRD Level 2\n\nFurther processed to align geo-referenced totals with nominal totals\n\nIncludes raising, strata updates, and handling of observer vs logbook data (e.g., IATTC)\n\nWarning: Not suitable for quota/legality studies due to uncertainty in precise georeferencing\n\nCode: https://github.com/firms-gta/geoflow-tunaatlas\n\nDOI: https://doi.org/10.5281/zenodo.15496164"
  },
  {
    "objectID": "datasetworkflowarchitecture-all.html#effort-dataset-ird-level-0",
    "href": "datasetworkflowarchitecture-all.html#effort-dataset-ird-level-0",
    "title": "Global Tuna Atlas datasets and workflow architecture",
    "section": "Effort Dataset (IRD Level 0)",
    "text": "Effort Dataset (IRD Level 0)\n\nTime span: 1950–2023\n\nContent: 23 different measurement units; no aggregation\n\nSpecificities: Known duplicates/parallel series (e.g., ICCAT Hours.FAD vs Hours.FSC; WCPFC SETS vs DAYS)-end-users must harmonize for their analysis\n\nDOI: https://doi.org/10.5281/zenodo.15496164"
  },
  {
    "objectID": "datasetworkflowarchitecture-all.html#exploration-resources",
    "href": "datasetworkflowarchitecture-all.html#exploration-resources",
    "title": "Global Tuna Atlas datasets and workflow architecture",
    "section": "Exploration resources",
    "text": "Exploration resources\n\nExample Rmd notebooks (in the L2/Effort repo):\n\nExploring Level 2 catch: https://github.com/firms-gta/geoflow-tunaatlas/blob/master/summary_catch.Rmd\n\nExploring Level 0 effort: https://github.com/firms-gta/geoflow-tunaatlas/blob/master/summary_effort.Rmd\n\nShiny apps:\n\nhttps://github.com/firms-gta/shiny_compare_fisheries_datasets\n\nhttps://github.com/firms-gta/tunaatlas_pie_map_shiny"
  },
  {
    "objectID": "deploy-repos-in-vlab-rstudioserver.html",
    "href": "deploy-repos-in-vlab-rstudioserver.html",
    "title": "Advanced: Deploy repositories in your own VLab",
    "section": "",
    "text": "This optional guide is for advanced users who want to understand how the VLab5 RStudio environment itself is deployed and how to reproduce something similar in their own VLab."
  },
  {
    "objectID": "deploy-repos-in-vlab-rstudioserver.html#deployment-of-rstudio-server-in-a-vlab",
    "href": "deploy-repos-in-vlab-rstudioserver.html#deployment-of-rstudio-server-in-a-vlab",
    "title": "Advanced: Deploy repositories in your own VLab",
    "section": "1) Deployment of RStudio Server in a VLab",
    "text": "1) Deployment of RStudio Server in a VLab\nThe deployment process is documented in this D4Science support ticket:\nhttps://support.d4science.org/issues/23536#note-15\nThe RStudio server setup for VLab5 is managed in the following repository:\nhttps://code-repo.d4science.org/gCubeSystem/rstudio-d4science/src/branch/fisheriesatlas-vlab5\nKey files include:\n\n07_vlab5_github_repositories.sh - defines which GitHub repositories (stable/dev) are preloaded into the environment.\nView script\nDockerfile - defines the RStudio server Docker image and how repositories and dependencies are installed.\nView Dockerfile\n\n\n⚠️ This is more technical and system-level. If you are interested in customizing or deploying your own RStudio VLab instance, we can schedule a dedicated walkthrough."
  },
  {
    "objectID": "deploy-repos-in-vlab-rstudioserver.html#managing-a-global-renv-cache-in-docker-images",
    "href": "deploy-repos-in-vlab-rstudioserver.html#managing-a-global-renv-cache-in-docker-images",
    "title": "Advanced: Deploy repositories in your own VLab",
    "section": "2) Managing a Global renv cache in Docker images",
    "text": "2) Managing a Global renv cache in Docker images\nWhen building custom RStudio Docker images for your own VLab, it is possible to preconfigure a shared global cache for renv. This ensures that all projects within the VLab use the same package cache, saving time and storage.\nThis is done by adding a line to .Renviron.site during Docker build:\nRUN echo 'RENV_PATHS_CACHE=\"~/blue-cloud-dataspace/GlobalFisheriesAtlas/cacheRenv\"' \\\n  >> \"${R_HOME}/etc/Renviron.site\"\nReference: https://support.d4science.org/issues/29739\n\nKey points:\n\nYou can set RENV_PATHS_CACHE to any directory, depending on your VLab setup.\n\nUsing ~/blue-cloud-dataspace/ is recommended: it is accessible across all users and all VLab instances (though slightly slower than local storage).\n\n.Renviron.site ensures the cache is global to the image - every repository opened in RStudio will reuse this shared cache.\n\nEffectively:\n\nEach repository still has its own renv.lock file (to ensure reproducibility).\n\nBut the package binaries are stored once in the shared cache, instead of reinstalling them for every project."
  },
  {
    "objectID": "deploy-repos-in-vlab-rstudioserver.html#the-truly-simple-path-what-to-consider",
    "href": "deploy-repos-in-vlab-rstudioserver.html#the-truly-simple-path-what-to-consider",
    "title": "Advanced: Deploy repositories in your own VLab",
    "section": "3) The truly simple path (what to consider)",
    "text": "3) The truly simple path (what to consider)\nIf you want to deploy the same setup as VLab5 in your own VLab, here are the essentials to look at and adapt per VLab:\n\nD4Science ticket (reference implementation & notes):\n\nSee: https://support.d4science.org/issues/23536#note-15\n(Contains key decisions & parameters used for the Fisheries Atlas VLab RStudio.)\n\nRStudio Server image for D4Science (to adapt):\n\nRepo: https://code-repo.d4science.org/gCubeSystem/rstudio-d4science/src/branch/fisheriesatlas-vlab5\nFiles to review:\n\nDockerfile\nhttps://code-repo.d4science.org/gCubeSystem/rstudio-d4science/src/branch/fisheriesatlas-vlab5/Dockerfile\n07_vlab5_github_repositories.sh (preloads the course repositories)\nhttps://code-repo.d4science.org/gCubeSystem/rstudio-d4science/src/branch/fisheriesatlas-vlab5/07_vlab5_github_repositories.sh\n\n\n\n\nIf this is interesting for your team, we can do a short technical walkthrough per VLab, since each environment may require minor adjustments (base image, permissions, storage mounts, etc.)."
  },
  {
    "objectID": "deploy-repos-in-vlab-rstudioserver.html#summary",
    "href": "deploy-repos-in-vlab-rstudioserver.html#summary",
    "title": "Advanced: Deploy repositories in your own VLab",
    "section": "4) Summary",
    "text": "4) Summary\n\nBasic users: You don’t need to worry about this - VLab5 is already configured.\nAdvanced users: You can adapt the Dockerfile and scripts for your own VLab deployment, preload specific repos, and configure a global renv cache.\n\nBest practice: Use a cache in blue-cloud-dataspace for shared accessibility across all VLab instances.\n\n\n✅ If you are interested in deploying or customizing your own RStudio environment with these settings, reach out - we can provide a step-by-step session on adapting the Dockerfile and repository scripts for your use case."
  },
  {
    "objectID": "derived_products.html",
    "href": "derived_products.html",
    "title": "Derived products",
    "section": "",
    "text": "This page summarizes derived products currently produced (or planned) from the Global Tuna Atlas workflows. These products build on the harmonized catch and effort datasets and enable analytical use cases (indicators, comparisons, reconciliations).\n\nStatus note: at present, no DOI has been assigned to these derived products. They are experimental/iterative/deprecated and may evolve."
  },
  {
    "objectID": "derived_products.html#cpue-catch-per-unit-effort",
    "href": "derived_products.html#cpue-catch-per-unit-effort",
    "title": "Derived products",
    "section": "CPUE (Catch Per Unit Effort)",
    "text": "CPUE (Catch Per Unit Effort)\nThe CPUE dataset is derived by combining catch and effort on a common analysis key (e.g., year × area × gear × species, optionally fishing_mode).\nFormula (per stratum):\nCPUE = Catch / Effort\nImportant considerations\n\nSome strata exist in only one input (catch or effort). Such strata are incomplete and are excluded or flagged during the join.\nThe unit depends on the effort measure (e.g., mt per 1000 hooks, mt per day, mt per set).\nMultiple CPUE variants are possible depending on the catch dataset used:\n\nCPUE from geo‑referenced catch (FIRMS L0 / IRD L2) + Effort L0.\nCPUE from nominal catch (for diagnostic purposes) + Effort L0.\n\nCurrent scripts (see CPUE step in the GTA launcher) handle joining and exporting standardized CWP‑compatible outputs.\n\nMinimal example (pseudo‑code)\nkeys <- c(\"time_start\",\"geographic_identifier\",\"gear_type\",\"species\")\njoined <- dplyr::inner_join(catch_tbl, effort_tbl, by = keys)\ncpue <- dplyr::mutate(joined,\n  cpue_value = measurement_value.x / measurement_value.y,\n  cpue_unit  = paste0(measurement_unit.x,\" per \", unique(measurement_unit.y))\n)"
  },
  {
    "objectID": "derived_products.html#level-1-harmonized-tons-numbers",
    "href": "derived_products.html#level-1-harmonized-tons-numbers",
    "title": "Derived products",
    "section": "Level 1 (Harmonized Tons & Numbers)",
    "text": "Level 1 (Harmonized Tons & Numbers)\nA Level 1 product harmonizes weights (tons) and numbers of fish onto a common structure, applying conversion factors where necessary.\nKey points\n\nProvides side‑by‑side tons and numbers for the same strata.\nRelies on species/gear/region‑specific conversion factors (length‑weight, dress‑to‑live weight, etc.).\nScientific caveat: results depend strongly on the chosen conversion factors and their provenance. Assumptions must be documented and sensitivity analyses are recommended.\n\nIntended use - Comparative analyses where both biomass and counts are informative (e.g., standardizing indices, bycatch assessments)."
  },
  {
    "objectID": "derived_products.html#crossdataset-analyses",
    "href": "derived_products.html#crossdataset-analyses",
    "title": "Derived products",
    "section": "Cross‑Dataset Analyses",
    "text": "Cross‑Dataset Analyses\nBeyond single indicators, several crossings are valuable for QA and interpretation:\n\nNominal vs Geo‑referenced (consistency checks):\n\nCompare totals by year/species/gear to identify gaps or raising impacts.\nMap differences to detect spatial/temporal mismatches.\n\nEffort unit diagnostics (per gear, per RFMO):\n\nInventory effort units by stratum; flag potential duplicates (same stratum, different units).\nDecide on project‑specific unit preferences before aggregating.\n\nRaising diagnostics (IRD L2):\n\nInspect strata where georeferenced data were raised to match nominal.\nVerify rules (exact match vs UNK/NEI/99.9) and year completeness constraints.\n\n\nOutput patterns\n\nSummary dashboards (Shiny) for exploratory review."
  },
  {
    "objectID": "exploring-global-tuna-atlas-data-via-shiny.html",
    "href": "exploring-global-tuna-atlas-data-via-shiny.html",
    "title": "Exploring Global Tuna Atlas data via Shiny",
    "section": "",
    "text": "Objective: Learn how to interact with processed GTA datasets through dynamic Shiny applications, understand the benefits/limits of each app, and how CWP-formatted datasets fit in.\nPrerequisite: Good understanding of GTA datasets (Levels, effort, CPUE).\nPresentation: Hands-on demo + self‑exploration + short quiz.\nOutcome: Learners can use a Shiny app to explore GTA datasets with or without writing code."
  },
  {
    "objectID": "exploring-global-tuna-atlas-data-via-shiny.html#quick-start-in-vlab5",
    "href": "exploring-global-tuna-atlas-data-via-shiny.html#quick-start-in-vlab5",
    "title": "Exploring Global Tuna Atlas data via Shiny",
    "section": "Quick start in VLab5",
    "text": "Quick start in VLab5\nShiny apps can be accessed in two ways:\n\nFrom the Shiny Apps tab in VLab → directly launch stable or dev versions of available apps.\nFrom the repository in ~/GitHubRepos → open the .Rproj, restore packages, and run shiny::runApp().\n\n\nAll Shiny apps have stable and dev versions, except darwin_core_viewer which is only available as a stable version in the Shiny Apps tab. Note: Depending on your level of write access, you may not be able to access the development version.\nIf you would like access, please contact us.\nPlease note that the list of deployed Shiny applications may evolve over time."
  },
  {
    "objectID": "exploring-global-tuna-atlas-data-via-shiny.html#outside-vlab-5",
    "href": "exploring-global-tuna-atlas-data-via-shiny.html#outside-vlab-5",
    "title": "Exploring Global Tuna Atlas data via Shiny",
    "section": "Outside Vlab 5",
    "text": "Outside Vlab 5\nIf you’re not (yet) a VLab5 member, you can still run the Shiny app locally using Docker.\n\nNote: Docker images can be large and the app may require several GB of RAM.\n\n\nWe strongly recommend running this Shiny app on VLab5 RStudio Server because it can require a lot of RAM (often 32 GB, depending on dataset size and filters). VLab5 provides:\n\n\n\na consistent environment (R/RStudio, system libs),\nhigher memory availability than many laptops,\nfewer installation issues (GDAL/PROJ, curl, etc. already present).\n\n\n\ndocker pull ghcr.io/<name-of-the-shiny-app-repo-eg.firms-gta/tunaatlas_pie_map_shiny> #(first time only)\ndocker run --rm --name tunaatlas \\\n  -p 3838:3838 \\\n  ghcr.io/<name-of-the-shiny-app-repo-eg.firms-gta/tunaatlas_pie_map_shiny>\nand then open http://127.0.0.1:3838/"
  },
  {
    "objectID": "exploring-global-tuna-atlas-data-via-shiny.html#shiny-apps-in-this-lesson",
    "href": "exploring-global-tuna-atlas-data-via-shiny.html#shiny-apps-in-this-lesson",
    "title": "Exploring Global Tuna Atlas data via Shiny",
    "section": "Shiny Apps in this lesson",
    "text": "Shiny Apps in this lesson\n\nMain apps (available in Shiny Apps tab)\n\ntunaatlas_pie_map_shiny (stable + dev)\nPurpose: Visualize any dataset in CWP format (catch/effort) with interactive maps and plots.\nGitHub: https://github.com/firms-gta/tunaatlas_pie_map_shiny Vlab5: https://blue-cloud.d4science.org/group/globalfisheriesatlas/global-tuna-atlas Docs: See repo README for supported schemas, required columns, and examples.\nshiny_compare_tunaatlas_datasets (stable + dev)\nPurpose: Compare harmonized GTA datasets (e.g., different Zenodo DOIs / releases / parameters).\nGitHub: https://github.com/firms-gta/tunaatlas_pie_map_shiny Vlab5: https://blue-cloud.d4science.org/group/globalfisheriesatlas/comparison-globaltunaatlas-datasets Docs: README describes how to reference DOIs and comparison keys.\n\n\n\nAdditional app\n\ndarwin_core_viewer (stable only)\nPurpose: Minimal viewer for Darwin Core biodiversity data (maps + plots). Good template for custom viewers.\nGitHub: https://github.com/firms-gta/darwin_core_viewer Vlab5: https://blue-cloud.d4science.org/group/globalfisheriesatlas/darwin-core-viewer Docs: README covers expected Darwin Core fields (e.g., scientificName, eventDate, decimalLatitude/Longitude)."
  },
  {
    "objectID": "exploring-global-tuna-atlas-data-via-shiny.html#example-walkthroughs",
    "href": "exploring-global-tuna-atlas-data-via-shiny.html#example-walkthroughs",
    "title": "Exploring Global Tuna Atlas data via Shiny",
    "section": "Example walkthroughs",
    "text": "Example walkthroughs\n\nA) Explore a CWP dataset with tunaatlas_pie_map_shiny\n\nFrom the Shiny Apps tab, open the stable version (recommended).\n(Alternatively, run locally with shiny::runApp() after restoring packages.)\n\nA formatted dataset is already loaded. You can as well choose a different dataset to explore in the ‘Choose dataset’ panel.\nFilter → by species, gear, year range, area.\n\nVisualize → interactive map (e.g., 5° grid) + plots; export as needed (see README).\n\nStrengths: quick exploration, map‑centric, supports broad CWP datasets.\nLimits: depends on schema conformity; heavy datasets may be slow.\n\n\nB) Compare several GTA releases with shiny_compare_tunaatlas_datasets\n\nFrom the Shiny Apps tab, open dev or stable version.\n\nDatasets are already loaded from DOIs. If you want to explore other datasets, just update the DOI.csv file in the repository.\n\nChoose keys (year/area/gear/species).\n\nInspect differences → tables/plots of differences, coverage, and changes.\n\nStrengths: release comparisons, quick differences across versions.\nLimits: assumes comparable schemas; interpret differencies carefully as the differences of processes for each datasets are complex."
  },
  {
    "objectID": "exploring-global-tuna-atlas-data-via-shiny.html#strengths-vs.-limitations-of-gui-tools",
    "href": "exploring-global-tuna-atlas-data-via-shiny.html#strengths-vs.-limitations-of-gui-tools",
    "title": "Exploring Global Tuna Atlas data via Shiny",
    "section": "Strengths vs. limitations of GUI tools",
    "text": "Strengths vs. limitations of GUI tools\nStrengths\n\nLower barrier to entry - no code needed.\n\nFast exploratory analysis & visual quality assurance.\n\nStandardized filters/views reduce errors.\n\nLimitations\n\nLess flexible than writing code for bespoke analytics.\n\nPerformance can degrade on very large datasets.\n\nSchema assumptions must be respected (CWP format)."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Fisheries Atlas - VLab5 Course",
    "section": "Welcome",
    "text": "Welcome\nThis website brings together tutorials and courses on the Global Tuna Atlas (GTA) datasets, the related Shiny applications, and how to use VLab5 within the Blue-Cloud 2026 project.\nOur aim is to help researchers, practitioners and decision-makers explore, analyse and reproduce global tuna fisheries workflows.\nYou will learn to:\n\nunderstand the structure and provenance of GTA datasets;\nrun analyses reproducibly on VLab5;\nuse GTA Shiny apps for exploration and QA/QC.\n\n\nAcknowledgement. This work is part of Blue-Cloud 2026 and has received funding from the European Union’s Horizon Europe programme (Grant Agreement No 101094227)."
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "Fisheries Atlas - VLab5 Course",
    "section": "Modules",
    "text": "Modules\n\nLesson 1 - Intro to Global Tuna Atlas\nThis lesson introduces the Global Tuna Atlas (GTA): its purpose, data levels (nominal, georeferenced, raised), and how it brings together datasets from all tuna RFMOs for global transparency and comparability.\n\n\n\nLesson 2 - Running R Workflows in VLab5\nHere you will learn how to run reproducible R workflows inside VLab5, using tools like renv, and understand how GTA workflows ensure consistency and collaboration.\n\n\n\nLesson 3 - Exploring GTA Data via Shiny\nThis lesson shows how to explore GTA datasets with Shiny apps: filtering by species, fleet, or gear, visualizing catches and effort through maps and time series, and exporting results easily."
  },
  {
    "objectID": "index.html#reuse-license",
    "href": "index.html#reuse-license",
    "title": "Fisheries Atlas - VLab5 Course",
    "section": "Reuse & license",
    "text": "Reuse & license\nMaterials are open for reuse unless noted otherwise."
  },
  {
    "objectID": "intro-rstudio.html",
    "href": "intro-rstudio.html",
    "title": "Introduction to VLab5 and RStudio Server",
    "section": "",
    "text": "This short notebook will guide you through the key features and technical setup of the RStudio Server in VLab5.\n\n\nTo begin working in VLab5 with R workflows, start by navigating to the Analytics. section on the platform.\nYou have two main options to launch an RStudio session:\n\nRStudio on D4Science (Start) – This is the default environment. It includes all preloaded repositories you’ll use in this course (both stable and dev versions). It has limited RAM compared to the second option.\nRStudio on GoogleCloud (Start) – This environment offers more RAM and computing power, which can be useful for heavier computations. However, it does not include the preloaded repositories by default. If you use it, you’ll need to manually configure and copy the necessary projects.\n\nOnce you launch either version, you’ll enter a standard RStudio Server interface directly in your browser.\nRStudio is organized into four panes:\n\nTop-left: Script editor\nBottom-left: Console and Terminal\n\nThe Console is where you type and run R code interactively.\nThe Terminal provides access to a Unix shell, useful for running system commands (e.g., Git, file management).\nThe Background Jobs tab (when visible) lets you launch long-running scripts without blocking the console. You can track progress and logs independently.\n\n\n\n🧩 These tabs are modular: you can show or hide panes using the menu bar → View > Panes or via the small gear icons in the top-right of each panel.\n\n\nTop-right: Environment, History, Git, Connections, Build, Tutorial\n\nThe Environment tab lists all current objects (datasets, models, functions).\nHistory tracks all commands you’ve run.\nGit appears if the project is under version control.\nBuild is used for R packages or Quarto/Bookdown projects.\nTutorial can show RStudio tutorials if available.\n\nBottom-right: Files, Plots, Packages, Help, and Viewer\n\nThis pane lets you navigate directories, visualize plots, install/view packages, access help pages, and render HTML outputs.\n\n\nTo explore the technical setup of your session, run the following lines of code in the R console:\n\n\n\n\n\n\nR.version.string\n\n[1] \"R version 4.2.3 (2023-03-15)\"\n\n\n\n\n\n\nrstudioapi::versionInfo()\n\nThis returns information such as:\n\nversion: RStudio version installed (e.g., 2023.3.0.386)\nrelease_name: Code name for the release (e.g., Cherry Blossom)\nmode: Whether you’re on the server version\n\n\n\n\n\nSys.info()\n\n                                                          sysname \n                                                          \"Linux\" \n                                                          release \n                                             \"4.15.0-189-generic\" \n                                                          version \n                   \"#200-Ubuntu SMP Wed Jun 22 19:53:37 UTC 2022\" \n                                                         nodename \n\"jupyter-bastien-2egrasset65011--rname-2d-52-53tudio-53erver-4fp\" \n                                                          machine \n                                                         \"x86_64\" \n                                                            login \n                                                        \"unknown\" \n                                                             user \n                                                         \"jovyan\" \n                                                   effective_user \n                                                         \"jovyan\" \n\n\nYou’ll see OS-level details such as:\n\nsysname: Operating system\nrelease: Kernel version\nmachine: Architecture (usually x86_64)\nuser: Your current user session (likely jovyan)\n\n\n\n\n# Note: this uses system-level shell command\nsystem(\"free -g\")\nThis command shows approximate memory allocation:\n\ntotal: Total memory available (e.g., 125 GB)\nused and available: What’s currently in use or free\n\n\n⚠️ Note: The number you see in “available” RAM may be lower (e.g., 32 GB) due to container quotas or dynamic allocation. The full infrastructure may have more memory, but your session may be limited based on demand and environment.\n📊 To get a user-friendly view of the actual allocated RAM, go to the top-right Environment panel in RStudio and click “Memory usage report”. This tool gives you a live snapshot of RAM used by your session.\n\n\n\n\n\ninstalled.packages()[1:20, c(\"Package\", \"Version\")]\n\n            Package       Version \nantiword    \"antiword\"    \"1.3.4\" \nbase64enc   \"base64enc\"   \"0.1-3\" \nbslib       \"bslib\"       \"0.7.0\" \ncachem      \"cachem\"      \"1.0.8\" \ncli         \"cli\"         \"3.6.2\" \ncommonmark  \"commonmark\"  \"2.0.0\" \ndigest      \"digest\"      \"0.6.35\"\nevaluate    \"evaluate\"    \"0.23\"  \nfastmap     \"fastmap\"     \"1.1.1\" \nfontawesome \"fontawesome\" \"0.5.2\" \nfs          \"fs\"          \"1.6.3\" \nglue        \"glue\"        \"1.7.0\" \nhighr       \"highr\"       \"0.10\"  \nhtmltools   \"htmltools\"   \"0.5.8\" \njquerylib   \"jquerylib\"   \"0.1.4\" \njsonlite    \"jsonlite\"    \"1.8.8\" \nknitr       \"knitr\"       \"1.45\"  \nlifecycle   \"lifecycle\"   \"1.0.4\" \nlitedown    \"litedown\"    \"0.7\"   \nmagrittr    \"magrittr\"    \"2.0.3\" \n\n\nThis shows the first 20 installed packages and their versions. You can increase the number for a full list.\n\n\n\n\n# Try loading a utility package\nrequire(antiword)\n\nLoading required package: antiword\n\n\nIf successful, this confirms that the package and its dependencies are correctly installed.\n\n\n\n\n\n\nAll environments and package lists may evolve over time.\nThe VLab infrastructure will be updated regularly to keep compatibility with the course materials.\nYou can always re-run this notebook to verify your session."
  },
  {
    "objectID": "lesson-3-5-overview.html",
    "href": "lesson-3-5-overview.html",
    "title": "Overview",
    "section": "",
    "text": "Estimated duration: 30 minutes\nFormat: lab notebook (Explainer video + real-time walkthrough incoming)\nPrerequisites: Access to VLab5, completion of VLab5 platform orientation."
  },
  {
    "objectID": "lesson-3-5-overview.html#course-map-pages",
    "href": "lesson-3-5-overview.html#course-map-pages",
    "title": "Overview",
    "section": "Course map (Pages)",
    "text": "Course map (Pages)\n\nIntro to RStudio in VLab5\nintro-rstudio.qmd - Launch options (D4Science vs Google Cloud), RStudio panes (Console, Terminal, Environment, Files/Plots/Packages/Help/Viewer), system checks (R/RStudio versions, RAM).\nProject Organization\nproject-organization.qmd - Pre-installed repositories in ~/GitHubRepos, stable vs dev structure, quick terminal command to list repos, README locations.\nRunning scripts in a loaded repository\nrunning-scripts-in-loaded-repo.qmd - Open .Rproj, run renv::restore(), render example Rmd (summary_catch.Rmd in geoflow-tunaatlas), pattern for Shiny apps (shiny::runApp()).\nDeploy repos in your own VLab (Bonus)\ndeploy-repos-in-vlab-rstudioserver.qmd - High-level notes for advanced users: RStudio Server image, preload script, global renv cache at image build-time, and optional per-project/per-user cache notes."
  },
  {
    "objectID": "lesson-3-5-overview.html#learning-objectives",
    "href": "lesson-3-5-overview.html#learning-objectives",
    "title": "Overview",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this lesson, learners will be able to:\n\nNavigate the VLab5 RStudio environment confidently.\nUnderstand the structure of project repositories, including stable vs dev versions.\nUse renv::restore() to initialize a reproducible R environment.\nExecute a complete R script within VLab5.\n(Bonus) Understand how the shared package cache works within the infrastructure.\n(Bonus) Deploy a repo in their own VLab environment if needed."
  },
  {
    "objectID": "lesson-3-5-overview.html#what-you-will-do",
    "href": "lesson-3-5-overview.html#what-you-will-do",
    "title": "Overview",
    "section": "What you will do",
    "text": "What you will do\n\nLaunch RStudio (D4Science or Google Cloud) and inspect the environment.\nExplore pre-installed repos in ~/GitHubRepos and locate main scripts.\nRestore packages with renv::restore() and run a full workflow (Rmd or Shiny).\nInspect outputs in outputs/ and review logs.\n(Bonus) Review how deployment works and how to configure a global cache."
  },
  {
    "objectID": "lesson-3-5-overview.html#expected-outcome",
    "href": "lesson-3-5-overview.html#expected-outcome",
    "title": "Overview",
    "section": "Expected outcome",
    "text": "Expected outcome\nBy the end of this lesson, learners will:\n\nIdentify and choose an appropriate repository.\nExecute a full R workflow in VLab5.\n(Bonus) Understand package dependencies and renv logic."
  },
  {
    "objectID": "project-organization.html",
    "href": "project-organization.html",
    "title": "Project organization",
    "section": "",
    "text": "All project repositories used in this course are pre-installed in your environment.\n\n\nProjects are located in:\ncd ~/GitHubRepos\nTo view the full list of repositories (including dev and stable subfolders), you can run the following in the Terminal:\nfind ~/GitHubRepos -maxdepth 2 -exec ls -ld {} \\;\nThis will show the folder structure and help you identify which projects are available.\n\n\n\n\nSome repositories include only a dev version → they are currently under development.\nOthers include both:\n\nstable/: the latest release from the main branch.\ndev/: the most recent push to the dev branch.\n\n\n\n\n\n\n\n\nRepository\nVersions\nDescription\nMain Script\nGitHub\n\n\n\n\ngeoflow-tunaatlas\nstable + dev\nPrepares the Global Tuna Atlas datasets (Levels 0, 1, 2; effort and CPUE) and deploys them to Zenodo. Also contains Dockerfiles for downloading source datasets and running workflows.\nlaunching_jsons_creating_GTA.R\nGitHub\n\n\ntunaatlas_pie_map_shiny\nstable + dev\nA Shiny app for visualizing any dataset in CWP format with various plots and maps.\nglobal.R\nGitHub\n\n\nshiny_compare_tunaatlas_datasets\ndev only\nA Shiny app to compare GTA datasets from zenodo DOIs.\nglobal.R\nGitHub\n\n\ndarwin_core_viewer\ndev only\nA basic Shiny app to visualize biodiversity data using the Darwin Core data format. Provides maps and plots. Designed as a starting point for custom apps.\napp.R\nGitHub\n\n\n\n\n📌 Documentation for each project is available in the README.md files within the folders, and on the respective GitHub repositories.\n\n\nYou can open and explore any of these projects directly in RStudio by navigating to the appropriate folder using the Files tab, or via setwd(\"~/GitHubRepos/<project>/<version>/<project>\") in your console.\nReady to run your first workflow? Jump to the next section Running Rscripts in a loaded project about the running of some scripts in the Vlab infrastructure of BlueCloud 2026."
  },
  {
    "objectID": "renv-cache-path.html",
    "href": "renv-cache-path.html",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "",
    "text": "This independent note explains how any RStudio Server user on the infrastructure can use a shared/global renv cache without modifying Docker images or course repositories."
  },
  {
    "objectID": "renv-cache-path.html#why-use-a-personal-renv-cache-setting",
    "href": "renv-cache-path.html#why-use-a-personal-renv-cache-setting",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Why use a personal renv cache setting?",
    "text": "Why use a personal renv cache setting?\n\nAvoid reinstalling the same packages for every project.\n\nSpeed up renv::restore() across projects by reusing binaries.\n\nKeep full reproducibility (each project still uses its own renv.lock)."
  },
  {
    "objectID": "renv-cache-path.html#option-a---set-a-user-level-cache-in-.renviron",
    "href": "renv-cache-path.html#option-a---set-a-user-level-cache-in-.renviron",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Option A - Set a user-level cache in ~/.Renviron",
    "text": "Option A - Set a user-level cache in ~/.Renviron\nCreate (or edit) a file named ~/.Renviron and add:\nRENV_PATHS_CACHE=~/blue-cloud-dataspace/GlobalFisheriesAtlas/cacheRenv\nThen restart R (Session → Restart R). From now on, all your projects will use this cache automatically.\nNotes:\n\nThe path can be any directory you can read/write.\n\nUsing a location under ~/blue-cloud-dataspace/ makes the cache accessible across workspaces on the infrastructure (shared but possibly a bit slower than other folders from the workspace).\nTo avoid issues caused by cache corruption or accidental deletions (since the cache folders are shared), you can isolate your project libraries from the mutual cache using:\n\nrenv::isolate()\nThis command copies all the required packages directly into the renv folder of your project. Be cautious when running this locally, as it can significantly increase disk usage."
  },
  {
    "objectID": "renv-cache-path.html#option-b---set-a-project-level-cache-in-project.renviron",
    "href": "renv-cache-path.html#option-b---set-a-project-level-cache-in-project.renviron",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Option B - Set a project-level cache in <project>/.Renviron",
    "text": "Option B - Set a project-level cache in <project>/.Renviron\nIf you prefer to scope the cache setting to a single project, create a file named .Renviron in the project root (next to .Rproj) with:\nRENV_PATHS_CACHE=~/blue-cloud-dataspace/GlobalFisheriesAtlas/cacheRenv\nRestart R inside the project. Only this project will use that cache."
  },
  {
    "objectID": "renv-cache-path.html#verify-your-configuration",
    "href": "renv-cache-path.html#verify-your-configuration",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Verify your configuration",
    "text": "Verify your configuration\nRun these checks in the R console:\n# What cache path is currently active?\nSys.getenv(\"RENV_PATHS_CACHE\")\n\n# Where does renv think the cache lives?\nrenv::paths$cache()\n\n# Project environment status (optional)\nrenv::status()\nIf you change ~/.Renviron or <project>/.Renviron, restart R to apply."
  },
  {
    "objectID": "renv-cache-path.html#using-the-cache-with-projects",
    "href": "renv-cache-path.html#using-the-cache-with-projects",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Using the cache with projects",
    "text": "Using the cache with projects\nInside any project (after opening the .Rproj):\nrenv::restore()   # will reuse packages from the shared cache when versions match\nPackages required by renv.lock will be linked or installed once into the cache and reused next time."
  },
  {
    "objectID": "renv-cache-path.html#good-practices-caveats",
    "href": "renv-cache-path.html#good-practices-caveats",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Good practices & caveats",
    "text": "Good practices & caveats\n\nKeep the cache path stable; moving it breaks existing links.\n\nIf the cache becomes very large, you can clean unused entries with renv::cache_clean() (advanced).\n\nThe cache is per R version; upgrading R may create a new cache tree.\n\nShared locations improve reuse across projects and users, but network storage can be slower than local disk."
  },
  {
    "objectID": "renv-cache-path.html#notes-on-group-specific-caches",
    "href": "renv-cache-path.html#notes-on-group-specific-caches",
    "title": "Advanced: Personal renv cache on RStudio Server (BlueCloud 2026 infrastructure)",
    "section": "Notes on group-specific caches",
    "text": "Notes on group-specific caches\nIn this course, we configured our cache in:\n~/blue-cloud-dataspace/GlobalFisheriesAtlas/cacheRenv\nbecause it is the one we use in the Vlab 5.\nBut you are free to define a different cache path that matches your own setup:\n\nA cache specific to your group of projects\nA cache specific to your VLab instance\n\nThis flexibility lets different groups share a cache internally while keeping separation from other workspaces."
  },
  {
    "objectID": "reproductibility.html#rstudio-server-on-vlab5",
    "href": "reproductibility.html#rstudio-server-on-vlab5",
    "title": "Why reproducibility matters in GTA workflows",
    "section": "RStudio Server on VLab5",
    "text": "RStudio Server on VLab5\nVLab5 provides a uniform RStudio environment (same RStudio version, OS image, system libs), which avoids “it works on my machine” issues and makes demos consistent across learners. Use it when you want:\n\nA pre-configured R/RStudio environment for everyone.\nStable system libraries (GDAL/PROJ, curl, etc.) aligned with the course.\nShared instructions: the same scripts and relative paths work for all users.\n\nAlternative: you can run exactly the same steps locally in RStudio; the workflows and scripts are identical."
  },
  {
    "objectID": "reproductibility.html#dois-and-zenodo",
    "href": "reproductibility.html#dois-and-zenodo",
    "title": "Why reproducibility matters in GTA workflows",
    "section": "DOIs and Zenodo",
    "text": "DOIs and Zenodo\nDOIs (Digital Object Identifiers) provide a permanent, citable identifier for each dataset release.\nIn GTA, every public release is archived on Zenodo, which mints:\n\na version DOI (e.g., 10.5281/zenodo.15496164) for that exact snapshot, and\na concept DOI (stable “latest” record) that always points to the newest version.\n\nWhy we use them\n\nStable citation in papers and reports,\nLong-term preservation of files and metadata,\nClear versioning for reproducibility (each run points to a specific DOI).\n\nHow to cite (example) Use the version DOI in methods/results, and the concept DOI in general references.\n\nGlobal Tuna Atlas (IRD release). Level-2 geo-referenced catch, Zenodo,\nDOI: 10.5281/zenodo.15496164 (versioned); concept DOI: 10.5281/zenodo.15496164\n\nBibTeX (template)\n@dataset{gta_level2_2025,\n  title   = {Global Tuna Atlas - Level 2 Geo-referenced Catch},\n  author  = {{GTA Team}},\n  year    = {2025},\n  version = {2025.1},\n  doi     = {10.5281/zenodo.15496164},\n  url     = {https://doi.org/10.5281/zenodo.15496164},\n  publisher = {Zenodo},\n  note    = {Use the version DOI for exact reproducibility}\n}"
  },
  {
    "objectID": "reproductibility.html#dependency-management-with-renv",
    "href": "reproductibility.html#dependency-management-with-renv",
    "title": "Why reproducibility matters in GTA workflows",
    "section": "Dependency management with renv",
    "text": "Dependency management with renv\nrenv captures the exact package versions used in the project (renv.lock), making analyses reproducible across sessions and machines. https://rstudio.github.io/renv/\nRestore the environment (recommended):\n\n# Restore all packages declared in renv.lock\nrenv::restore()"
  },
  {
    "objectID": "reproductibility.html#docker-images",
    "href": "reproductibility.html#docker-images",
    "title": "Why reproducibility matters in GTA workflows",
    "section": "Docker images",
    "text": "Docker images\nThe following datasets: Level 2 geo‑referenced, effort ship a Dockerfile that encapsulates:\n\nCode (repositories + pinned versions),\nR packages (via renv), and\nBuild scripts that produce the final dataset.\n\nThis guarantees that a dataset release can be recreated bit‑for‑bit provided the same inputs.\nAt this stage, Level 0 and effort datasets still rely on datasets manually collected from tRFMO websites, some of which are stored in a private Google Drive folder and not publicly accessible. Consequently, only Level 2 datasets can currently be fully built using open DOI-based sources, without relying on restricted files."
  },
  {
    "objectID": "running-scripts-in-loaded-repo.html",
    "href": "running-scripts-in-loaded-repo.html",
    "title": "Running Rscripts in a loaded project",
    "section": "",
    "text": "Before running any code in a repository, make sure you are inside the correct R project (.Rproj). This activates project-specific settings (including renv) and sets the working directory properly."
  },
  {
    "objectID": "running-scripts-in-loaded-repo.html#step-1---open-the-project-.rproj",
    "href": "running-scripts-in-loaded-repo.html#step-1---open-the-project-.rproj",
    "title": "Running Rscripts in a loaded project",
    "section": "Step 1 - Open the project (.Rproj)",
    "text": "Step 1 - Open the project (.Rproj)\nOpen a project using one of these options:\n\nFrom the Files pane (bottom-right): navigate to the repository folder and click the .Rproj file.\nProgrammatically (requires rstudioapi):\n\nif (!requireNamespace(\"rstudioapi\", quietly = TRUE)) install.packages(\"rstudioapi\")\nrstudioapi::openProject(\"~/GitHubRepos/geoflow-tunaatlas/stable\")\n\nOpening the .Rproj ensures that renv is activated via the project’s .Rprofile and that your working directory is correct."
  },
  {
    "objectID": "running-scripts-in-loaded-repo.html#step-2---restore-the-project-environment-with-renv",
    "href": "running-scripts-in-loaded-repo.html#step-2---restore-the-project-environment-with-renv",
    "title": "Running Rscripts in a loaded project",
    "section": "Step 2 - Restore the Project environment with renv",
    "text": "Step 2 - Restore the Project environment with renv\nThe renv package manages project-specific R environments to guarantee reproducibility. It records exact package versions in renv.lock and restores them on any machine.\nRun this once per project:\nrenv::restore()\nWhat this does:\n\nInstalls missing packages or links them from VLab’s shared cache (fast).\nAligns your session’s packages with the versions expected by the project.\n\nUseful tips:\n# Check environment status (optional)\nrenv::status()\n\n# If prompted after updates, restart your R session\nif (requireNamespace(\"rstudioapi\", quietly = TRUE)) rstudioapi::restartSession()\n\n✅ Always run renv::restore() when you open a new repository.\nℹ️ A later section of this course dives deeper into renv, shared caches in VLab, and linking multiple repos to a common cache directory.\n\nIf you want to use the cache renv with your own projects or within your Vlab feel free to follow this tutorial renv cache path."
  },
  {
    "objectID": "running-scripts-in-loaded-repo.html#example-with-geoflow-tunaatlas-project",
    "href": "running-scripts-in-loaded-repo.html#example-with-geoflow-tunaatlas-project",
    "title": "Running Rscripts in a loaded project",
    "section": "Example with geoflow-tunaatlas project",
    "text": "Example with geoflow-tunaatlas project\n\nStep 3: Explore the .Rmd report\nThe file summary_catch.Rmd is an R Markdown document that allows you to explore catch datasets and generate visual outputs like maps and plots. A similar file is also available for effort data.\nYou can render the entire report at once using:\nrmarkdown::render(\"summary_catch.Rmd\")\nHowever, for learning purposes, we recommend exploring the file chunk-by-chunk to understand each step in the workflow.\nOpen the file in the RStudio editor and review each code block, which typically includes: - Data loading from zenodo, or in the VLab from existing folder (to avoid re-downloading data) - Data transformation and filtering - Visualization with maps and plots - Output generation\n\n\nOutputs\nExecuting the full .Rmd will generate an HTML report and may also create visual outputs or data files, often saved in the same directory or an outputs/ subfolder.\nNote: geoflow-tunaatlas is the most complex repository used in this course. It contains several scripts, workflows, and Docker-related files beyond the .Rmd example shown here."
  },
  {
    "objectID": "running-scripts-in-loaded-repo.html#other-repositories",
    "href": "running-scripts-in-loaded-repo.html#other-repositories",
    "title": "Running Rscripts in a loaded project",
    "section": "Other repositories",
    "text": "Other repositories\nFor all of the other repositories in this course:\n\ntunaatlas_pie_map_shiny\nshiny_compare_tunaatlas_datasets\ndarwin_core_viewer the usage is much simpler. You typically only need two commands to get started:\n\nrenv::restore()\nshiny::runApp()\nThese repositories are Shiny apps that can be launched directly after restoring the environment.\nYou’re now ready to explore and execute workflows and Shiny apps across all repositories in VLab!"
  }
]